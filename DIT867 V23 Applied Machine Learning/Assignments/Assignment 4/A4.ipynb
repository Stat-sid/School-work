{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 0.94 sec.\n",
      "Accuracy: 0.7919.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from aml_perceptron import Perceptron, SparsePerceptron\n",
    "\n",
    "# This function reads the corpus, returns a list of documents, and a list\n",
    "# of their corresponding polarity labels. \n",
    "def read_data(corpus_file):\n",
    "    X = []\n",
    "    Y = []\n",
    "    with open(corpus_file, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            _, y, _, x = line.split(maxsplit=3)\n",
    "            X.append(x.strip())\n",
    "            Y.append(y)\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "# Read all the documents.\n",
    "X, Y = read_data('data/all_sentiment_shuffled.txt')\n",
    "\n",
    "# Split into training and test parts.\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.2,\n",
    "                                                random_state=0)\n",
    "\n",
    "# Set up the preprocessing steps and the classifier.\n",
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(),\n",
    "    SelectKBest(k=1000),\n",
    "    Normalizer(),\n",
    "\n",
    "    # NB that this is our Perceptron, not sklearn.linear_model.Perceptron\n",
    "    Perceptron()  \n",
    ")\n",
    "\n",
    "# Train the classifier.\n",
    "t0 = time.time()\n",
    "pipeline.fit(Xtrain, Ytrain)\n",
    "t1 = time.time()\n",
    "print('Training time: {:.2f} sec.'.format(t1-t0))\n",
    "\n",
    "# Evaluate on the test set.\n",
    "Yguess = pipeline.predict(Xtest)\n",
    "print('Accuracy: {:.4f}.'.format(accuracy_score(Ytest, Yguess)))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file shows a couple of implementations of the perceptron learning\n",
    "algorithm. It is based on the code from Lecture 3, but using the slightly\n",
    "more compact perceptron formulation that we saw in Lecture 6.\n",
    "\n",
    "There are two versions: Perceptron, which uses normal NumPy vectors and\n",
    "matrices, and SparsePerceptron, which uses sparse vectors and matrices.\n",
    "The latter may be faster when we have high-dimensional feature representations\n",
    "with a lot of zeros, such as when we are using a \"bag of words\" representation\n",
    "of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 1.33 sec.\n",
      "Accuracy: 0.8238.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class LinearClassifier(BaseEstimator):\n",
    "    \"\"\"\n",
    "    General class for binary linear classifiers. Implements the predict\n",
    "    function, which is the same for all binary linear classifiers. There are\n",
    "    also two utility functions.\n",
    "    \"\"\"\n",
    "\n",
    "    def decision_function(self, X: np.array):\n",
    "        \"\"\"\n",
    "        Computes the decision function for the inputs X. The inputs are assumed to be\n",
    "        stored in a matrix, where each row contains the features for one\n",
    "        instance.\n",
    "        \"\"\"\n",
    "        return X.dot(self.w)\n",
    "\n",
    "    def predict(self, X: np.array):\n",
    "        \"\"\"\n",
    "        Predicts the outputs for the inputs X. The inputs are assumed to be\n",
    "        stored in a matrix, where each row contains the features for one\n",
    "        instance.\n",
    "        \"\"\"\n",
    "\n",
    "        # First compute the output scores\n",
    "        scores = self.decision_function(X)\n",
    "\n",
    "        # Select the positive or negative class label, depending on whether\n",
    "        # the score was positive or negative.\n",
    "        out = np.select([scores >= 0.0, scores < 0.0],\n",
    "                        [self.positive_class,\n",
    "                         self.negative_class])\n",
    "        return out\n",
    "\n",
    "    def find_classes(self, Y):\n",
    "        \"\"\"\n",
    "        Finds the set of output classes in the output part Y of the training set.\n",
    "        If there are exactly two classes, one of them is associated to positive\n",
    "        classifier scores, the other one to negative scores. If the number of\n",
    "        classes is not 2, an error is raised.\n",
    "        \"\"\"\n",
    "        classes = sorted(set(Y))\n",
    "        if len(classes) != 2:\n",
    "            raise Exception(\"this does not seem to be a 2-class problem\")\n",
    "        self.positive_class = classes[1]\n",
    "        self.negative_class = classes[0]\n",
    "\n",
    "    def encode_outputs(self, Y):\n",
    "        \"\"\"\n",
    "        A helper function that converts all outputs to +1 or -1.\n",
    "        \"\"\"\n",
    "        return np.array([1 if y == self.positive_class else -1 for y in Y])\n",
    "\n",
    "\n",
    "\n",
    "class Perceptron(LinearClassifier):\n",
    "    \"\"\"\n",
    "    A straightforward implementation of the perceptron learning algorithm.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_iter=20):\n",
    "        \"\"\"\n",
    "        The constructor can optionally take a parameter n_iter specifying how\n",
    "        many times we want to iterate through the training set.\n",
    "        \"\"\"\n",
    "        self.n_iter = n_iter\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Train a linear classifier using the perceptron learning algorithm.\n",
    "        \"\"\"\n",
    "\n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        # Convert all outputs to +1 (for the positive class) or -1 (negative).\n",
    "        Y = self.encode_outputs(Y)\n",
    "\n",
    "        # If necessary, convert the sparse matrix returned by a vectorizer\n",
    "        # into a normal NumPy matrix.\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = X.toarray()\n",
    "\n",
    "        # Initialize the weight vector to all zeros.\n",
    "        n_features = X.shape[1]\n",
    "        self.w = np.zeros(n_features)\n",
    "\n",
    "        # Perceptron algorithm:\n",
    "        for i in range(self.n_iter):\n",
    "            for x, y in zip(X, Ye):\n",
    "\n",
    "                # Compute the output score for this instance.\n",
    "                score = x.dot(self.w)\n",
    "\n",
    "                # If there was an error, update the weights.\n",
    "                if y*score <= 0:\n",
    "                    self.w += y*x\n",
    "\n",
    "class SVM(LinearClassifier):\n",
    "    def __init__(self, lambda_value=0.001, n_iter=20) -> None:\n",
    "        self.n_iter = n_iter\n",
    "        self.lambda_value = lambda_value\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        Ye = self.encode_outputs(Y)\n",
    "    \n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = X.toarray()\n",
    "        \n",
    "        self.w = np.zeros(X.shape[1])\n",
    "        t = 0\n",
    "\n",
    "        #Pegasos\n",
    "        for i in range(self.n_iter):\n",
    "            for x, y in zip(X, Ye):\n",
    "                t = t + 1\n",
    "\n",
    "                eta = 1/(self.lambda_value * t)\n",
    "                score = self.w @ x \n",
    "\n",
    "                if (y * score) < 1:\n",
    "                    self.w = (1-eta * self.lambda_value) * self.w + (eta*y) * x\n",
    "                else:\n",
    "                    self.w = (1-eta * self.lambda_value) * self.w\n",
    "\n",
    "# Set up the preprocessing steps and the classifier.\n",
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(),\n",
    "    SelectKBest(k=1000),\n",
    "    Normalizer(),\n",
    "\n",
    "    SVM()  \n",
    ")\n",
    "\n",
    "# Train the classifier.\n",
    "t0 = time.time()\n",
    "pipeline.fit(Xtrain, Ytrain)\n",
    "t1 = time.time()\n",
    "print('Training time: {:.2f} sec.'.format(t1-t0))\n",
    "SVM_time = t1-t0\n",
    "\n",
    "# Evaluate on the test set.\n",
    "Yguess = pipeline.predict(Xtest)\n",
    "print('Accuracy: {:.4f}.'.format(accuracy_score(Ytest, Yguess)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 1.71 sec.\n",
      "Accuracy: 0.8082.\n"
     ]
    }
   ],
   "source": [
    "class Log_reg(LinearClassifier):\n",
    "    def __init__(self, lambda_value=0.001, n_iter=20) -> None:\n",
    "        self.n_iter = n_iter\n",
    "        self.lambda_value = lambda_value\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        Ye = self.encode_outputs(Y)\n",
    "    \n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = X.toarray()\n",
    "        \n",
    "    \n",
    "        self.w = np.zeros(X.shape[1])\n",
    "        t = 0\n",
    "\n",
    "        #Pegasos\n",
    "        for i in range(self.n_iter):\n",
    "            for x, y in zip(X, Ye):\n",
    "                t = t + 1\n",
    "\n",
    "                eta = 1/(self.lambda_value * t)\n",
    "                score = self.w @ x \n",
    "                gradient = -(y / (1 + np.exp(y * score))) * x\n",
    "\n",
    "                self.w = self.w - eta*gradient\n",
    "                self.w = (1-eta * self.lambda_value) * self.w\n",
    "\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(),\n",
    "    SelectKBest(k=1000),\n",
    "    Normalizer(),\n",
    "\n",
    "    Log_reg()  \n",
    ")\n",
    "\n",
    "# Train the classifier.\n",
    "t0 = time.time()\n",
    "pipeline.fit(Xtrain, Ytrain)\n",
    "t1 = time.time()\n",
    "print('Training time: {:.2f} sec.'.format(t1-t0))\n",
    "Log_reg_time = t1-t0\n",
    "\n",
    "# Evaluate on the test set.\n",
    "Yguess = pipeline.predict(Xtest)\n",
    "print('Accuracy: {:.4f}.'.format(accuracy_score(Ytest, Yguess)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making it faster with BLAS (or, making it BLASingly fast)\n",
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 1.337 sec, without BLAS it took 1.329\n",
      "Accuracy: 0.8238.\n"
     ]
    }
   ],
   "source": [
    "from scipy.linalg import blas\n",
    "class BLASLinearClassifier(LinearClassifier):\n",
    "    def decision_function(self, X: np.array):\n",
    "        return blas.ddot(X, self.w)\n",
    "\n",
    "class BLASSVM(LinearClassifier):\n",
    "    def __init__(self, lambda_value=0.001, n_iter=20) -> None:\n",
    "        self.n_iter = n_iter\n",
    "        self.lambda_value = lambda_value\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        Ye = self.encode_outputs(Y)\n",
    "    \n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = X.toarray()\n",
    "        \n",
    "        self.w = np.zeros(X.shape[1])\n",
    "        t = 0\n",
    "\n",
    "        #Pegasos\n",
    "        for i in range(self.n_iter):\n",
    "            for x, y in zip(X, Ye):\n",
    "                t = t + 1\n",
    "\n",
    "                eta = 1/(self.lambda_value * t)\n",
    "                score = blas.ddot(self.w, x) \n",
    "\n",
    "                if (y * score) < 1:\n",
    "                    # self.w = self.w * (1-eta * self.lambda_value) + ((eta*y) * x) # I think this COULD be BLAS:ed, but I dont understand how\n",
    "                    blas.dscal((1-eta * self.lambda_value), self.w)\n",
    "                    blas.daxpy(((eta*y) * x), self.w, a=1)\n",
    "\n",
    "                else:\n",
    "                    blas.dscal((1-eta * self.lambda_value), self.w)\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(),\n",
    "    SelectKBest(k=1000),\n",
    "    Normalizer(),\n",
    "\n",
    "    BLASSVM()  \n",
    ")\n",
    "\n",
    "# Train the classifier.\n",
    "t0 = time.time()\n",
    "pipeline.fit(Xtrain, Ytrain)\n",
    "t1 = time.time()\n",
    "print(f'Training time: {round(t1-t0, 3)} sec, without BLAS it took {round(SVM_time, 3)}')\n",
    "\n",
    "# Evaluate on the test set.\n",
    "Yguess = pipeline.predict(Xtest)\n",
    "print('Accuracy: {:.4f}.'.format(accuracy_score(Ytest, Yguess)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 1.385 sec, without BLAS it took 1.712\n",
      "Accuracy: 0.4964.\n"
     ]
    }
   ],
   "source": [
    "class BLASLog_reg(LinearClassifier):\n",
    "    def __init__(self, lambda_value=0.001, n_iter=20) -> None:\n",
    "        self.n_iter = n_iter\n",
    "        self.lambda_value = lambda_value\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        Ye = self.encode_outputs(Y)\n",
    "    \n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = X.toarray()\n",
    "        \n",
    "        self.w = np.zeros(X.shape[1])\n",
    "        t = 0\n",
    "\n",
    "        #Pegasos\n",
    "        for i in range(self.n_iter):\n",
    "            for x, y in zip(X, Ye):\n",
    "                t = t + 1\n",
    "\n",
    "                eta = 1/(self.lambda_value * t)\n",
    "                score = blas.ddot(self.w, x) \n",
    "                gradient = -(y / (1 + np.exp(y * score))) * x\n",
    "\n",
    "                blas.daxpy(gradient, self.w, -eta)                  #Something wrong here\n",
    "                #self.w = self.w - eta*gradient\n",
    "                \n",
    "                blas.dscal(self.w, (1-eta * self.lambda_value))     # or here\n",
    "                #self.w = (1-eta * self.lambda_value) * self.w\n",
    "\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(),\n",
    "    SelectKBest(k=1000),\n",
    "    Normalizer(),\n",
    "\n",
    "    BLASLog_reg()  \n",
    ")\n",
    "\n",
    "# Train the classifier.\n",
    "t0 = time.time()\n",
    "pipeline.fit(Xtrain, Ytrain)\n",
    "t1 = time.time()\n",
    "print(f'Training time: {round(t1-t0, 3)} sec, without BLAS it took {round(Log_reg_time, 3)}')\n",
    "\n",
    "# Evaluate on the test set.\n",
    "Yguess = pipeline.predict(Xtest)\n",
    "print('Accuracy: {:.4f}.'.format(accuracy_score(Ytest, Yguess)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# b) Making it sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Ye' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 70\u001b[0m\n\u001b[0;32m     62\u001b[0m pipeline \u001b[39m=\u001b[39m make_pipeline(\n\u001b[0;32m     63\u001b[0m     TfidfVectorizer(),\n\u001b[0;32m     64\u001b[0m     \u001b[39m#SelectKBest(k=1000),\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     67\u001b[0m     Perceptron()  \n\u001b[0;32m     68\u001b[0m )\n\u001b[0;32m     69\u001b[0m t0 \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m---> 70\u001b[0m pipeline\u001b[39m.\u001b[39;49mfit(Xtrain, Ytrain)\n\u001b[0;32m     71\u001b[0m t1 \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m     72\u001b[0m perceptron_time \u001b[39m=\u001b[39m t1\u001b[39m-\u001b[39mt0\n",
      "File \u001b[1;32mc:\\Users\\efsts\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:405\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    403\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_final_estimator \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpassthrough\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    404\u001b[0m         fit_params_last_step \u001b[39m=\u001b[39m fit_params_steps[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m0\u001b[39m]]\n\u001b[1;32m--> 405\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_final_estimator\u001b[39m.\u001b[39mfit(Xt, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params_last_step)\n\u001b[0;32m    407\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "Cell \u001b[1;32mIn[11], line 92\u001b[0m, in \u001b[0;36mPerceptron.fit\u001b[1;34m(self, X, Y)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[39m# Perceptron algorithm:\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_iter):\n\u001b[1;32m---> 92\u001b[0m     \u001b[39mfor\u001b[39;00m x, y \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(X, Ye):\n\u001b[0;32m     93\u001b[0m \n\u001b[0;32m     94\u001b[0m         \u001b[39m# Compute the output score for this instance.\u001b[39;00m\n\u001b[0;32m     95\u001b[0m         score \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mdot(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mw)\n\u001b[0;32m     97\u001b[0m         \u001b[39m# If there was an error, update the weights.\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Ye' is not defined"
     ]
    }
   ],
   "source": [
    "##### The following part is for the optional task.\n",
    "\n",
    "### Sparse and dense vectors don't collaborate very well in NumPy/SciPy.\n",
    "### Here are two utility functions that help us carry out some vector\n",
    "### operations that we'll need.\n",
    "\n",
    "def add_sparse_to_dense(x, w, factor):\n",
    "    \"\"\"\n",
    "    Adds a sparse vector x, scaled by some factor, to a dense vector.\n",
    "    This can be seen as the equivalent of w += factor * x when x is a dense\n",
    "    vector.\n",
    "    \"\"\"\n",
    "    w[x.indices] += factor * x.data\n",
    "\n",
    "def sparse_dense_dot(x, w):\n",
    "    \"\"\"\n",
    "    Computes the dot product between a sparse vector x and a dense vector w.\n",
    "    \"\"\"\n",
    "    return np.dot(w[x.indices], x.data)\n",
    "\n",
    "\n",
    "class SparsePerceptron(LinearClassifier):\n",
    "    def __init__(self, n_iter=20):\n",
    "        \"\"\"\n",
    "        The constructor can optionally take a parameter n_iter specifying how\n",
    "        many times we want to iterate through the training set.\n",
    "        \"\"\"\n",
    "        self.n_iter = n_iter\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Train a linear classifier using the perceptron learning algorithm.\n",
    "\n",
    "        Note that this will only work if X is a sparse matrix, such as the\n",
    "        output of a scikit-learn vectorizer.\n",
    "        \"\"\"\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        Ye = self.encode_outputs(Y)\n",
    "\n",
    "        # Initialize the weight vector to all zeros.\n",
    "        self.w = np.zeros(X.shape[1])\n",
    "\n",
    "        # Iteration through sparse matrices can be a bit slow, so we first\n",
    "        # prepare this list to speed up iteration.\n",
    "        XY = list(zip(X, Ye))\n",
    "\n",
    "        for i in range(self.n_iter):\n",
    "            for x, y in XY:\n",
    "\n",
    "                # Compute the output score for this instance.\n",
    "                # (This corresponds to score = x.dot(self.w) above.)\n",
    "                score = sparse_dense_dot(x, self.w)\n",
    "\n",
    "                # If there was an error, update the weights.\n",
    "                if y*score <= 0:\n",
    "                    # (This corresponds to self.w += y*x above.)\n",
    "                    add_sparse_to_dense(x, self.w, y)\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(),\n",
    "    #SelectKBest(k=1000),\n",
    "    Normalizer(),\n",
    "\n",
    "    Perceptron()  \n",
    ")\n",
    "t0 = time.time()\n",
    "pipeline.fit(Xtrain, Ytrain)\n",
    "t1 = time.time()\n",
    "perceptron_time = t1-t0\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(),\n",
    "    #SelectKBest(k=1000),\n",
    "    Normalizer(),\n",
    "\n",
    "    SparsePerceptron()  \n",
    ")\n",
    "\n",
    "# Train the classifier.\n",
    "t0 = time.time()\n",
    "pipeline.fit(Xtrain, Ytrain)\n",
    "t1 = time.time()\n",
    "print(f'Training time: {round(t1-t0, 3)} sec, without sparse matricies it took {round(perceptron_time, 3)}')\n",
    "\n",
    "# Evaluate on the test set.\n",
    "Yguess = pipeline.predict(Xtest)\n",
    "print('Accuracy: {:.4f}.'.format(accuracy_score(Ytest, Yguess)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iter = 0\n",
      "Entered 1\n",
      "1000.0\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 62\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[39m# Train the classifier.\u001b[39;00m\n\u001b[0;32m     61\u001b[0m t0 \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m---> 62\u001b[0m pipeline\u001b[39m.\u001b[39;49mfit(Xtrain, Ytrain)\n\u001b[0;32m     63\u001b[0m t1 \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m     64\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTraining time: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mround\u001b[39m(t1\u001b[39m-\u001b[39mt0,\u001b[39m \u001b[39m\u001b[39m3\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m sec, without sparse matricies it took \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mround\u001b[39m(SVM_time,\u001b[39m \u001b[39m\u001b[39m3\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\efsts\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:405\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    403\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_final_estimator \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpassthrough\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    404\u001b[0m         fit_params_last_step \u001b[39m=\u001b[39m fit_params_steps[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m0\u001b[39m]]\n\u001b[1;32m--> 405\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_final_estimator\u001b[39m.\u001b[39mfit(Xt, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params_last_step)\n\u001b[0;32m    407\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "Cell \u001b[1;32mIn[9], line 29\u001b[0m, in \u001b[0;36mSparseSVM.fit\u001b[1;34m(self, X, Y)\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[39m#self.w = (1-eta * self.lambda_value) * self.w + (eta*y) * x\u001b[39;00m\n\u001b[0;32m     27\u001b[0m     \u001b[39m#add_sparse_to_dense() this needs to be converted to add_sparse_to_dense?\u001b[39;00m\n\u001b[0;32m     28\u001b[0m     \u001b[39mprint\u001b[39m(eta\u001b[39m*\u001b[39my)\n\u001b[1;32m---> 29\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mw \u001b[39m=\u001b[39m (\u001b[39m1\u001b[39m\u001b[39m-\u001b[39meta \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlambda_value) \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mw \u001b[39m+\u001b[39m sparse_dense_dot(x, eta\u001b[39m*\u001b[39;49my)\n\u001b[0;32m     31\u001b[0m     \u001b[39m# (This corresponds to self.w += y*x above.)\u001b[39;00m\n\u001b[0;32m     32\u001b[0m     \u001b[39m#add_sparse_to_dense(x, self.w, y)\u001b[39;00m\n\u001b[0;32m     33\u001b[0m     \u001b[39m#self.w = self.w * (1-eta * self.lambda_value) + ((eta*y) * x) \u001b[39;00m\n\u001b[0;32m     34\u001b[0m \n\u001b[0;32m     35\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     36\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mEntered 2\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[8], line 19\u001b[0m, in \u001b[0;36msparse_dense_dot\u001b[1;34m(x, w)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msparse_dense_dot\u001b[39m(x, w):\n\u001b[0;32m     16\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[39m    Computes the dot product between a sparse vector x and a dense vector w.\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mdot(w[x\u001b[39m.\u001b[39;49mindices], x\u001b[39m.\u001b[39mdata)\n",
      "\u001b[1;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "class SparseSVM(LinearClassifier):\n",
    "    def __init__(self, lambda_value=0.001, n_iter=20) -> None:\n",
    "        self.n_iter = n_iter\n",
    "        self.lambda_value = lambda_value\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        Ye = self.encode_outputs(Y)\n",
    "        \n",
    "        self.w = np.zeros(X.shape[1])\n",
    "        t = 0\n",
    "\n",
    "        #Pegasos\n",
    "        XY = list(zip(X, Ye))\n",
    "        for i in range(self.n_iter):\n",
    "            print(f\"n_iter = {i}\")\n",
    "            for x, y in XY:\n",
    "                t = t + 1\n",
    "\n",
    "                eta = 1/(self.lambda_value * t)\n",
    "                score = sparse_dense_dot(x, self.w)\n",
    "\n",
    "                if (y * score) < 1:\n",
    "                    print(\"Entered 1\")\n",
    "                    #self.w = (1-eta * self.lambda_value) * self.w + (eta*y) * x\n",
    "                    #add_sparse_to_dense() this needs to be converted to add_sparse_to_dense?\n",
    "                    print(eta*y)\n",
    "                    self.w = (1-eta * self.lambda_value) * self.w + sparse_dense_dot(x, eta*y)\n",
    "                    \n",
    "                    # (This corresponds to self.w += y*x above.)\n",
    "                    #add_sparse_to_dense(x, self.w, y)\n",
    "                    #self.w = self.w * (1-eta * self.lambda_value) + ((eta*y) * x) \n",
    "\n",
    "                else:\n",
    "                    print(\"Entered 2\")\n",
    "                    #self.w = (1-eta * self.lambda_value) * self.w\n",
    "                    self.w = sparse_dense_dot(self.w, (1-eta * self.lambda_value)) \n",
    "\n",
    "#pipeline = make_pipeline(\n",
    "#    TfidfVectorizer(),\n",
    "#    #SelectKBest(k=1000),\n",
    "#    Normalizer(),\n",
    "#\n",
    "#    SVM()  \n",
    "#)\n",
    "#t0 = time.time()\n",
    "#pipeline.fit(Xtrain, Ytrain)\n",
    "#t1 = time.time()\n",
    "#SVM_time = t1-t0\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(),\n",
    "    #SelectKBest(k=1000),\n",
    "    Normalizer(),\n",
    "\n",
    "    SparseSVM()  \n",
    ")\n",
    "\n",
    "# Train the classifier.\n",
    "t0 = time.time()\n",
    "pipeline.fit(Xtrain, Ytrain)\n",
    "t1 = time.time()\n",
    "print(f'Training time: {round(t1-t0, 3)} sec, without sparse matricies it took {round(SVM_time, 3)}')\n",
    "\n",
    "# Evaluate on the test set.\n",
    "Yguess = pipeline.predict(Xtest)\n",
    "print('Accuracy: {:.4f}.'.format(accuracy_score(Ytest, Yguess)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "89819e1d67d96602d7ae8598ec2b2f4f6b2cd5be6b4333e6ae2d1768cbea82d4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
